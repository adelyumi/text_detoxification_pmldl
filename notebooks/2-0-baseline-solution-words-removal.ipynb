{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b2979d2e",
   "metadata": {},
   "source": [
    "# Baseline solution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a85fe4ba",
   "metadata": {},
   "source": [
    "Here I will implement a baseline solution: simple \"toxic\" words removal.\n",
    "\n",
    "The list of toxic words is taken from https://github.com/LDNOOBW/List-of-Dirty-Naughty-Obscene-and-Otherwise-Bad-Words/blob/master/en."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4d5d4318",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/external/en.txt') as file:\n",
    "    badwords = [line.rstrip() for line in file]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c5e7dade",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['2g1c',\n",
       " '2 girls 1 cup',\n",
       " 'acrotomophilia',\n",
       " 'alabama hot pocket',\n",
       " 'alaskan pipeline',\n",
       " 'anal',\n",
       " 'anilingus',\n",
       " 'anus',\n",
       " 'apeshit',\n",
       " 'arsehole']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "badwords[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38b27aa0",
   "metadata": {},
   "source": [
    "Do not need training here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "328a4241",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reference</th>\n",
       "      <th>translation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Fucking A your mom likes lan.</td>\n",
       "      <td>my mom loves you.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>We'll be fucking pariahs.</td>\n",
       "      <td>we're going to be completely unnerved.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I'm done, Live Dead.</td>\n",
       "      <td>I'm through, Dead Meat.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>What is this place? A fucking vampire secret h...</td>\n",
       "      <td>that's a secret vampire headquarters.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Just a silly dream and nothing more</td>\n",
       "      <td># Just a silky dream and nothing more</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           reference  \\\n",
       "0                      Fucking A your mom likes lan.   \n",
       "1                          We'll be fucking pariahs.   \n",
       "2                               I'm done, Live Dead.   \n",
       "3  What is this place? A fucking vampire secret h...   \n",
       "4                Just a silly dream and nothing more   \n",
       "\n",
       "                              translation  \n",
       "0                       my mom loves you.  \n",
       "1  we're going to be completely unnerved.  \n",
       "2                 I'm through, Dead Meat.  \n",
       "3   that's a secret vampire headquarters.  \n",
       "4   # Just a silky dream and nothing more  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "test_data = pd.read_csv('../data/interim/test.csv')\n",
    "test_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d3b5d73",
   "metadata": {},
   "source": [
    "## Toxic words removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1cfd90cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from tqdm import tqdm\n",
    "\n",
    "def remove_toxic_words(texts):\n",
    "    clean_texts = []\n",
    "    \n",
    "    for i in tqdm(range(len(texts))):\n",
    "        regex = re.compile(r\"\\b\" + r\"\\b|\\b\".join(map(re.escape, badwords)) + r\"\\b\", re.IGNORECASE)\n",
    "        clean_texts.append(regex.sub(\"\", texts[i]))\n",
    "\n",
    "    return clean_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5f27ab14",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████| 85352/85352 [01:02<00:00, 1364.69it/s]\n"
     ]
    }
   ],
   "source": [
    "pred = remove_toxic_words(list(test_data.reference))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "766828b3",
   "metadata": {},
   "source": [
    "## Algorithm performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87aa71bd",
   "metadata": {},
   "source": [
    "### Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f445dcd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████| 85352/85352 [02:30<00:00, 566.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average similarity: 0.9780516129612405\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "count_vectorizer = CountVectorizer(token_pattern=r\"(?u)\\b\\w+\\b\")\n",
    "cosine_sims = []\n",
    "\n",
    "for i in tqdm(range(len(test_data))):\n",
    "    texts = [test_data.reference[i], pred[i]]\n",
    "    vector_matrix = count_vectorizer.fit_transform(texts)\n",
    "\n",
    "    cosine_sims.append(cosine_similarity(vector_matrix)[0][1])\n",
    "    \n",
    "print(\"Average similarity:\", sum(cosine_sims) / len(cosine_sims))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81d83812",
   "metadata": {},
   "source": [
    "The similarity is very high because we don't add other words to the text, only remove toxic ones. But the logic of some of the sentences is broken."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ba9731c",
   "metadata": {},
   "source": [
    "## Examples of work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e8876475",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reference: Doesn't anybody in this town speak in complete fucking sentences anymore?\n",
      "Detoxifyed: Doesn't anybody in this town speak in complete  sentences anymore?\n",
      "\n",
      "Reference: you even tried to wipe your butt off.\n",
      "Detoxifyed: you even tried to wipe your  off.\n",
      "\n",
      "Reference: My eyes are fucked up.\n",
      "Detoxifyed: My eyes are fucked up.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in [8, 58, 650]: \n",
    "    print(\"Reference:\", test_data.reference[i])\n",
    "    print(\"Detoxifyed:\", pred[i])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cefe526b",
   "metadata": {},
   "source": [
    "What can be seen from some examples:\n",
    "1. The first example shows the successful work of the remover. This is the case when a toxic word can be simply removed without losing the meaning and logic of the sentence.\n",
    "2. The second example just cut out a word and now the sentence has lost its logical meaning.\n",
    "3. The remover is limited to only a list of bad words, which means that not all toxic words will be removed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "824171ab",
   "metadata": {},
   "source": [
    "## Conclusions:\n",
    "The idea of this algorithm is very simple, so the obtained results are expected.\n",
    "\n",
    "The algorithm did not significantly reduce the toxicity of sentences due to the limited word list and the inability to recognize toxic context. Similarity to the original sentences remains high, but the sentences themselves often lose grammatical meaning."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
